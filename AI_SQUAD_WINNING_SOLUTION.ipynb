{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## AI4D-LAB-TANZANIA-TOURISM-CLASSIFICATION-CHALLENGE"
      ],
      "metadata": {
        "id": "nCQaQ0Ygl1kA"
      },
      "id": "nCQaQ0Ygl1kA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you use tourism survey data and ML to classify the range of expenditures a tourist spends in Tanzania?\n",
        "\n",
        "The Tanzanian tourism sector plays a significant role in the Tanzanian economy, contributing about 17% to the country’s GDP and 25% of all foreign exchange revenues. The sector, which provides direct employment for more than 600,000 people and up to 2 million people indirectly, generated approximately $2.4 billion in 2018 according to government statistics. Tanzania received a record 1.1 million international visitor arrivals in 2014, mostly from Europe, the US and Africa.\n",
        "\n",
        "Tanzania is the only country in the world which has allocated more than 25% of its total area for wildlife, national parks, and protected areas.There are 16 national parks in Tanzania, 28 game reserves, 44 game-controlled areas, two marine parks and one conservation area.\n",
        "\n",
        "Tanzania’s tourist attractions include the Serengeti plains, which hosts the largest terrestrial mammal migration in the world; the Ngorongoro Crater, the world’s largest intact volcanic caldera and home to the highest density of big game in Africa; Kilimanjaro, Africa’s highest mountain; and the Mafia Island marine park; among many others. The scenery, topography, rich culture and very friendly people provide for excellent cultural tourism, beach holidays, honeymooning, game hunting, historical and archaeological ventures – and certainly the best wildlife photography safaris in the world.\n",
        "\n",
        "The objective of this hackathon is to develop a machine learning model that can classify the range of expenditures a tourist spends in Tanzania. The model can be used by different tour operators and the Tanzania Tourism Board to automatically help tourists across the world estimate their expenditure before visiting Tanzania.\n",
        "\n",
        "https://zindi.africa/competitions/ai4d-lab-tanzania-tourism-classification-challenge"
      ],
      "metadata": {
        "id": "0hHturVuuI3d"
      },
      "id": "0hHturVuuI3d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI Squad Team Members\n",
        "\n",
        ">@Ebiendele (Team Leader), <br>@Mike_ade, <br>@D-Prof"
      ],
      "metadata": {
        "id": "WaOHeREWceki"
      },
      "id": "WaOHeREWceki"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Dataset from zindi using the Zindi package"
      ],
      "metadata": {
        "id": "4RkOB9iVc9pm"
      },
      "id": "4RkOB9iVc9pm"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip -q install git+https://github.com/eaedk/testing-zindi-package.git\n",
        "# from zindi.user import Zindian\n",
        "# USERNAME = \"adetoromichael346@gmail.com\" #@param {type : \"string\"}\n",
        "# user = Zindian(username = USERNAME)\n",
        "# user.select_a_challenge(reward = 'all', kind = 'competition', active = 'true')\n",
        "# user.download_dataset(destination = \"dataset\")\n",
        "# I can't find the AI4D... competition on the list of challenges, so i will manually import the data."
      ],
      "metadata": {
        "id": "cevlDI-KcsmN"
      },
      "id": "cevlDI-KcsmN",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4bf04e1-4424-4b3e-930f-09cb2389fc44"
      },
      "source": [
        "### LOAD NECESSARY LIBRARIES"
      ],
      "id": "f4bf04e1-4424-4b3e-930f-09cb2389fc44"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost optuna # installing catboost and optuna libraries"
      ],
      "metadata": {
        "id": "GeQqb-7zdQAD"
      },
      "id": "GeQqb-7zdQAD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "f3b75b56-fe77-4c6e-adf8-346221e04299"
      },
      "outputs": [],
      "source": [
        "# dependencies\n",
        "import re\n",
        "import pandas as pd\n",
        "pd.options.display.max_columns = 999\n",
        "pd.options.display.max_rows = 999\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold\n",
        "from catboost import Pool, CatBoostClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from lightgbm import LGBMClassifier\n",
        "from geopy.geocoders import Nominatim\n",
        "import optuna\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "f3b75b56-fe77-4c6e-adf8-346221e04299"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYheXgaTaz7K",
        "outputId": "94b85a20-6951-46b3-f034-f8ed7f626489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@markdown <br><center><img src='https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Google_Drive_logo.png/600px-Google_Drive_logo.png' height=\"150\" alt=\"Gdrive-logo\"/></center>\n",
        "#@markdown <center><h2>Mount GDrive to /content/drive</h3></center><br>\n",
        "MODE = \"MOUNT\" #@param [\"MOUNT\", \"UNMOUNT\"]\n",
        "#Mount your Gdrive! \n",
        "from google.colab import drive\n",
        "drive.mount._DEBUG = False\n",
        "if MODE == \"MOUNT\":\n",
        "  drive.mount('/content/drive', force_remount=True)\n",
        "elif MODE == \"UNMOUNT\":\n",
        "  try:\n",
        "    drive.flush_and_unmount()\n",
        "  except ValueError:\n",
        "    pass\n",
        "  get_ipython().system_raw(\"rm -rf /root/.config/Google/DriveFS\")"
      ],
      "id": "lYheXgaTaz7K"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4433f51d-343c-4eee-928e-5c8ab8ad265d"
      },
      "source": [
        "### LOAD DATA (and other relevant dataset like continent and latitude and longitude)"
      ],
      "id": "4433f51d-343c-4eee-928e-5c8ab8ad265d"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d7e94678-c4a2-4665-b07d-f15e60bf6dd1"
      },
      "outputs": [],
      "source": [
        "# load the data\n",
        "path = '/content/drive/MyDrive/DATASET/dataset_/AI4D DATA'\n",
        "# path to the folder where the datasets are located in my google drive\n",
        "cont_coun = pd.read_csv(f'{path}/Countries-Continents.csv')\n",
        "# downloaded from 'https://github.com/dbouquin/IS_608/blob/master/NanosatDB_munging/Countries-Continents.csv'\n",
        "lat_lon = pd.read_csv(f'{path}/world_country_and_usa_states_latitude_and_longitude_values.csv')\n",
        "# downloaded from 'https://www.kaggle.com/datasets/paultimothymooney/latitude-and-longitude-for-every-country-and-state'\n",
        "Train = pd.read_csv(f'{path}/Train.csv')\n",
        "Test = pd.read_csv(f'{path}/Test.csv')\n",
        "sub = pd.read_csv(f'{path}/SampleSubmission.csv')\n",
        "random_seed = 2001 # random seed for all computations"
      ],
      "id": "d7e94678-c4a2-4665-b07d-f15e60bf6dd1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c8d0eb5-dff9-4eb2-8f1d-510d444b0b0a"
      },
      "source": [
        "### COMBINE DATA INTO A WHOLE DATASET"
      ],
      "id": "3c8d0eb5-dff9-4eb2-8f1d-510d444b0b0a"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7e9e07b-8594-4f9d-aa44-024081e8a11f",
        "outputId": "6c6c5799-044c-42ba-dfe2-e02f8ddb67e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24675, 21)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "all_data = pd.concat([Train, Test], sort = False).reset_index(drop = True)\n",
        "all_data.shape"
      ],
      "id": "e7e9e07b-8594-4f9d-aa44-024081e8a11f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57c1d752-484b-4fdb-9225-62749708873c"
      },
      "source": [
        "### PREPROCESSING"
      ],
      "id": "57c1d752-484b-4fdb-9225-62749708873c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will fill missing values in travel with with free-bus, and not alone, even though where travel with is missing, the addition of total_male and total_female gives us 1 for almost all cases which signifies that they are most likely alone. This is because there must be a reason why it is missing even though they were alone during the tourism."
      ],
      "metadata": {
        "id": "ZjBJuHFEd2gx"
      },
      "id": "ZjBJuHFEd2gx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since total_male and total female have missing values, this means no male or female tourist were around during that particular tourism, so i will fill the missing values with 0"
      ],
      "metadata": {
        "id": "Hn6XoLabdixk"
      },
      "id": "Hn6XoLabdixk"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "51a63a61-fdaf-416e-85a9-37a29a25c209"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()\n",
        "def preprocess(data):\n",
        "  data.travel_with.fillna('Free-bus', inplace = True)\n",
        "  data.total_female.fillna(0, inplace = True)\n",
        "  data.total_male.fillna(0, inplace = True)\n",
        "  # label encoding packages, tour_arrangement and first_trip_tz\n",
        "  LE_cols = ['package_transport_int', 'package_accomodation', 'package_food', 'tour_arrangement',\\\n",
        "                  'package_transport_tz', 'package_sightseeing', 'package_guided_tour', 'package_insurance', 'first_trip_tz']\n",
        "  for le_col in LE_cols:\n",
        "      data[le_col] = le.fit_transform(data[le_col])\n",
        "  return data\n",
        "\n",
        "preprocessed_data = preprocess(all_data)"
      ],
      "id": "51a63a61-fdaf-416e-85a9-37a29a25c209"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29d15191-b6a2-4b48-af3e-dce9730e2735"
      },
      "source": [
        "### FEATURE INTERACTION AND ENGINEERING"
      ],
      "id": "29d15191-b6a2-4b48-af3e-dce9730e2735"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_ucoWYrUHFf",
        "outputId": "0d9d2e13-e4e3-470c-aa57-332b1b14f330"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((18506, 65), (6169, 65))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "def feature_engineering(data, cont_coun, lat_lon):\n",
        "  # mapping continent to country\n",
        "  data['country_'] = data['country'].str.title()\n",
        "  cont_coun.rename(columns = {'Country' : 'country_'}, inplace = True)\n",
        "  data.country_.replace({'United States Of America' : 'US', 'Drc' : 'Congo, Democratic Republic of', 'Swizerland' : 'Switzerland', 'Morroco' : 'Morocco', 'Uae': 'United Arab Emirates', 'Saud Arabia' : 'Saudi Arabia', 'Myanmar' : 'Burma (Myanmar)', 'Russia' : 'Russian Federation',\\\n",
        "                                  'Korea' : 'Korea, South', 'Czech Republic' : 'CZ', 'Taiwan' : 'China', 'Djibout' : 'Djibouti', 'Ukrain' : 'Ukraine', 'Malt' : 'Malta', 'Costarica' : 'Costa Rica', 'Burgaria' : 'Bulgaria', 'Comoro' : 'Comoros', 'Philipines' : 'Philippines', 'Somali' : 'Somalia',\\\n",
        "                                  'Ecuado' : 'Ecuador', 'Monecasque' : 'Monaco', 'Trinidad Tobacco' : 'Trinidad and Tobago', 'Bosnia' : 'Bosnia and Herzegovina'}, inplace=True)\n",
        "  data = data.merge(cont_coun, on = 'country_', how = 'left')\n",
        "  data.Continent = np.where((data.country_ == 'Scotland') & (data.Continent.isna()), 'Europe', data.Continent)\n",
        "  data.Continent = np.where((data.country_ == 'Bermuda') & (data.Continent.isna()), 'North America', data.Continent)\n",
        "  data.drop(['country_'], 1, inplace = True)  \n",
        "\n",
        "  # mapping latitude and longitude to country\n",
        "  lat_lon = lat_lon[['country', 'latitude', 'longitude']]\n",
        "  data['country'] = data['country'].str.title()\n",
        "  data.country.replace({'United States Of America' : 'United States', 'Drc' : 'Congo [DRC]', 'Congo' : 'Congo [Republic]', 'Swizerland' : 'Switzerland', 'Morroco' : 'Morocco', 'Uae': 'United Arab Emirates', 'Saud Arabia' : 'Saudi Arabia', 'Myanmar' : 'Myanmar [Burma]', \\\n",
        "                                  'Korea' : 'South Korea', 'Ivory Coast' : 'Côte d\\'Ivoire', 'Djibout' : 'Djibouti', '\\tDjibouti' : 'Djibouti', 'Ukrain' : 'Ukraine', 'Malt' : 'Malta', 'Costarica' : 'Costa Rica', 'Burgaria' : 'Bulgaria', 'Comoro' : 'Comoros', 'Philipines' : 'Philippines', 'Somali' : 'Somalia', \\\n",
        "                                  'Ecuado' : 'Ecuador', 'Macedonia' : 'Macedonia [FYROM]', 'Monecasque' : 'Monaco', 'Trinidad Tobacco' : 'Trinidad and Tobago', 'Bosnia' : 'Bosnia and Herzegovina', 'Scotland' : ''}, inplace=True)\n",
        "  data = data.merge(lat_lon, on = 'country', how = 'left')\n",
        "  data.latitude.fillna(56.78611112, inplace = True)\n",
        "  data.longitude.fillna(-4.1140518, inplace = True)\n",
        "  \n",
        "  # mapping age\n",
        "  map_age = {'<18' : 18, '18-24' : 24, '25-44' : 44, '45-64' : 64, '65+' : 75}\n",
        "  data.age_group = data.age_group.map(map_age)\n",
        "  \n",
        "  # feature interaction\n",
        "  # adding nights together\n",
        "  data['total_nights'] = data.night_mainland + data.night_zanzibar \n",
        "  # adding packages together\n",
        "  data['total_packages'] = data.package_transport_int + data.package_accomodation + data.package_food + data.package_transport_tz + data.package_sightseeing + data.package_guided_tour + data.package_insurance\n",
        "  # adding people together\n",
        "  data['total_people'] = data.total_male + data.total_female\n",
        "  # dividing packages by people available\n",
        "  data[\"packages_per_people\"] = data[\"total_packages\"] / data[\"total_people\"]\n",
        "\n",
        "  # frequecy encoding, since it has many unique features\n",
        "  cols = ['country']\n",
        "  for col in cols:\n",
        "    data[col] = data[col].map(data.groupby(col).size() / len(data))\n",
        "  \n",
        "  # Groupby features by mean\n",
        "  data['country_by_people'] = data['total_people'].groupby(data['country']).transform('mean')\n",
        "  data['people_by_packages'] = data['total_packages'].groupby(data['total_people']).transform('mean')\n",
        "  data['people_by_night'] = data['total_nights'].groupby(data['total_people']).transform('mean')\n",
        "  data['age_by_packages'] = data['total_packages'].groupby(data['age_group']).transform('mean')\n",
        "  \n",
        "  # one hot encoding\n",
        "  cols2dum = ['info_source', 'main_activity', 'purpose', 'travel_with', 'Continent']\n",
        "  data = pd.get_dummies(data, prefix_sep = '_', columns = cols2dum)\n",
        "  \n",
        "  # handling inf values and missing values\n",
        "  data = data.replace([np.inf], np.nan)\n",
        "  data.fillna(data.mean() , inplace = True)\n",
        "  \n",
        "  # clustering the dataset into 6 different clusters since we have 6 different classes to classify\n",
        "  data_km = data.drop(['Tour_ID', 'cost_category'], axis = 1)\n",
        "  km = KMeans(n_clusters = 6, random_state = random_seed)\n",
        "  data['cluster'] = km.fit_predict(data_km)\n",
        "  \n",
        "  # renaming column names, by removing char in the string\n",
        "  data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x), inplace=True)\n",
        "  data.drop(['Tour_ID'], 1, inplace = True)\n",
        "  \n",
        "  # getting train and test dataset\n",
        "  train = data[data.cost_category.notnull()].reset_index(drop = True)\n",
        "  test = data[data.cost_category.isna()].reset_index(drop = True)\n",
        "  return train, test\n",
        "\n",
        "train, test = feature_engineering(preprocessed_data, cont_coun, lat_lon)\n",
        "train.shape, test.shape"
      ],
      "id": "T_ucoWYrUHFf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e66484d-3d17-4c23-99a0-0f949f49b22d"
      },
      "source": [
        "### MODELLING and evaluation"
      ],
      "id": "7e66484d-3d17-4c23-99a0-0f949f49b22d"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2550ba65-23fa-40af-9605-034f207c215a"
      },
      "outputs": [],
      "source": [
        "X, y = train.drop('cost_category', axis = 1), train['cost_category']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .15, shuffle = True, random_state = random_seed)"
      ],
      "id": "2550ba65-23fa-40af-9605-034f207c215a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "853a9d6e-e4c1-4cc4-95eb-8a090260130e"
      },
      "source": [
        "#### BASELINE MODEL"
      ],
      "id": "853a9d6e-e4c1-4cc4-95eb-8a090260130e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab191435-9942-4c30-9428-8921ae465ffc"
      },
      "source": [
        "##### CATBOOST"
      ],
      "id": "ab191435-9942-4c30-9428-8921ae465ffc"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e389060c-d551-48ba-bfe2-98962d5b2e0b",
        "outputId": "b099a165-8827-4c51-9735-719e0792de85"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.03045"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "d_train, d_test  = Pool(X_train, y_train), Pool(X_test, y_test)\n",
        "cb_model_ = CatBoostClassifier(l2_leaf_reg = 9.441413522475084, depth = 7, bootstrap_type = 'Bayesian', learning_rate = 0.01772339213540557, n_estimators = 3167, use_best_model = True,\n",
        "                                                 leaf_estimation_iterations = 1, random_strength = 0.17095032711212016, loss_function = 'MultiClass', verbose = 0, random_state = random_seed)\n",
        "cb_model_.fit(d_train, eval_set = [(d_test)], verbose = 0, early_stopping_rounds = 500)\n",
        "preds_ = cb_model_.predict_proba(d_test)\n",
        "log_loss(y_test, preds_).round(5)"
      ],
      "id": "e389060c-d551-48ba-bfe2-98962d5b2e0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c93c2b8-f8ac-4cfc-868d-6af7fec7f7ee"
      },
      "source": [
        "##### LIGHTGBM"
      ],
      "id": "9c93c2b8-f8ac-4cfc-868d-6af7fec7f7ee"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "717d5c95-4349-4ea2-a9c1-6f6b65bc8d07",
        "outputId": "d9372027-de2b-4955-8c76-6edb0d222b08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.03165"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "lgb_model_ = LGBMClassifier(boosting_type = 'gbdt', objective = 'multiclass', metric = 'multi_logloss', n_estimators = 3000, learning_rate = 0.01, use_best_model = True,\n",
        "                                             num_leaves = 45, colsample_bytree = 0.5, subsample = 0.9, subsample_freq = 1, max_depth = 6, reg_alpha = 0.8, reg_lambda = 0.8,\n",
        "                                             min_split_gain = 0.05, min_child_weight = 0.05, random_state = random_seed, num_class = 6, silent = -1, verbose = -1)\n",
        "lgb_model_.fit(X_train, y_train, eval_set = [(X_train, y_train), (X_test, y_test)], early_stopping_rounds = 500, eval_metric = 'logloss', verbose = 0)\n",
        "preds_ = lgb_model_.predict_proba(X_test)\n",
        "log_loss(y_test, preds_).round(5)"
      ],
      "id": "717d5c95-4349-4ea2-a9c1-6f6b65bc8d07"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d667d53c-b946-4d1c-b53c-4d4e78443acc"
      },
      "source": [
        "### CROSS-VALIDATION"
      ],
      "id": "d667d53c-b946-4d1c-b53c-4d4e78443acc"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "tgo6a9wftp89"
      },
      "outputs": [],
      "source": [
        "# creating fols to n=be used for cross validation\n",
        "TARGET_COL = 'cost_category'\n",
        "remove_features = ['cost_category', 'folds']\n",
        "features_columns = [col for col in train.columns if col not in remove_features]\n",
        "cat = le.fit_transform(train.cost_category)\n",
        "def create_folds(data):\n",
        "    data[\"folds\"] = -1\n",
        "    data = data.sample(frac = 1).reset_index(drop = True)\n",
        "    num_bins = np.floor(1 + np.log2(len(train))).astype(int)\n",
        "    data.loc[:, \"bins\"] = pd.cut(cat, bins = num_bins, labels = False)\n",
        "    kf = StratifiedKFold(n_splits = 15)\n",
        "    for f, (t_, v_) in enumerate(kf.split(X = data, y = data.bins.values)):\n",
        "        data.loc[v_, \"folds\"] = f\n",
        "    data.drop(\"bins\", axis = 1, inplace = True)\n",
        "    return data\n",
        "train = create_folds(train)"
      ],
      "id": "tgo6a9wftp89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c78f6e6-93b5-4102-b570-7ce57af6dfe0"
      },
      "source": [
        "##### CATBOOST"
      ],
      "id": "5c78f6e6-93b5-4102-b570-7ce57af6dfe0"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d911d9b-b26e-4adf-9954-e3ff34e5fea4",
        "outputId": "41969752-63d1-4e3a-f088-0e3a92ed8581"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "LOG_LOSS_1: 1.0557587942913207\n",
            "------------------------------\n",
            "LOG_LOSS_2: 1.0925862059968245\n",
            "------------------------------\n",
            "LOG_LOSS_3: 1.0402501930017034\n",
            "------------------------------\n",
            "LOG_LOSS_4: 1.034107627439311\n",
            "------------------------------\n",
            "LOG_LOSS_5: 1.0593547324176908\n",
            "------------------------------\n",
            "LOG_LOSS_6: 1.0549207403493077\n",
            "------------------------------\n",
            "LOG_LOSS_7: 1.0407737579244611\n",
            "------------------------------\n",
            "LOG_LOSS_8: 1.0869126688266508\n",
            "------------------------------\n",
            "LOG_LOSS_9: 1.0344020700567562\n",
            "------------------------------\n",
            "LOG_LOSS_10: 1.0552004593679924\n",
            "------------------------------\n",
            "LOG_LOSS_11: 1.0397300234388287\n",
            "------------------------------\n",
            "LOG_LOSS_12: 1.0344587168314696\n",
            "------------------------------\n",
            "LOG_LOSS_13: 1.0602824873411716\n",
            "------------------------------\n",
            "LOG_LOSS_14: 1.0524757646426146\n",
            "------------------------------\n",
            "LOG_LOSS_15: 1.0534895988112714\n",
            "------------------------------\n",
            "\n",
            "\n",
            "LOG_LOSS_CV_CB: 1.05298\n"
          ]
        }
      ],
      "source": [
        "log_loss_score_ = []\n",
        "print(\"-\" * 30)\n",
        "n_splits = 15\n",
        "for fold in range(n_splits):\n",
        "  x_train_, y_train_ = train[train['folds']!=fold][features_columns] , train[train['folds']!=fold][TARGET_COL] \n",
        "  x_test_, y_test_ = train[train['folds']==fold][features_columns] , train[train['folds']==fold][TARGET_COL] \n",
        "  d_train = Pool(x_train_, y_train_)\n",
        "  d_test = Pool(x_test_, y_test_)\n",
        "  model_cb = CatBoostClassifier(l2_leaf_reg = 9.441413522475084, depth = 7, bootstrap_type = 'Bayesian', learning_rate = 0.01772339213540557, n_estimators = 3167, use_best_model = True,\n",
        "                                                 leaf_estimation_iterations = 1, random_strength = 0.17095032711212016, loss_function = 'MultiClass', verbose = 0, random_state = random_seed)\n",
        "  model_cb.fit(d_train, eval_set = [(d_train), (d_test)], verbose = 0, early_stopping_rounds = 500)\n",
        "  preds_ = model_cb.predict_proba(d_test)\n",
        "  log_loss_ = log_loss(y_test_, preds_)\n",
        "  print(f'LOG_LOSS_{fold + 1}: {log_loss_}')\n",
        "  log_loss_score_.append(log_loss_)\n",
        "  print(\"-\" * 30)\n",
        "print('\\n')\n",
        "print(f\"LOG_LOSS_CV_CB: {np.mean(log_loss_score_).round(5)}\")"
      ],
      "id": "1d911d9b-b26e-4adf-9954-e3ff34e5fea4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fe1e2b1-9888-4341-a976-3722a031a4b6"
      },
      "source": [
        "##### LIGHTGBM"
      ],
      "id": "7fe1e2b1-9888-4341-a976-3722a031a4b6"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "207c6fb7-82a3-4b28-919c-13f7d7e02bcd",
        "outputId": "6fb52592-315a-48eb-a328-3103fc27f8c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "LOG_LOSS_1: 1.0620548180422837\n",
            "------------------------------\n",
            "LOG_LOSS_2: 1.09405293911991\n",
            "------------------------------\n",
            "LOG_LOSS_3: 1.0400865489768625\n",
            "------------------------------\n",
            "LOG_LOSS_4: 1.0370939084571578\n",
            "------------------------------\n",
            "LOG_LOSS_5: 1.0607797717165917\n",
            "------------------------------\n",
            "LOG_LOSS_6: 1.060595525158828\n",
            "------------------------------\n",
            "LOG_LOSS_7: 1.0423544809683352\n",
            "------------------------------\n",
            "LOG_LOSS_8: 1.094199925132832\n",
            "------------------------------\n",
            "LOG_LOSS_9: 1.0407448597036435\n",
            "------------------------------\n",
            "LOG_LOSS_10: 1.058403559178775\n",
            "------------------------------\n",
            "LOG_LOSS_11: 1.0510352523487851\n",
            "------------------------------\n",
            "LOG_LOSS_12: 1.0388229842521257\n",
            "------------------------------\n",
            "LOG_LOSS_13: 1.0672714542995443\n",
            "------------------------------\n",
            "LOG_LOSS_14: 1.055973995534415\n",
            "------------------------------\n",
            "LOG_LOSS_15: 1.0514247197482978\n",
            "------------------------------\n",
            "\n",
            "\n",
            "LOG_LOSS_CV_LGB: 1.05699\n"
          ]
        }
      ],
      "source": [
        "log_loss_score_lgb = []\n",
        "print(\"-\" * 30)\n",
        "n_splits = 15\n",
        "for fold in range(n_splits):\n",
        "  x_train_, y_train_ = train[train['folds']!=fold][features_columns] , train[train['folds']!=fold][TARGET_COL] \n",
        "  x_test_, y_test_ = train[train['folds']==fold][features_columns] , train[train['folds']==fold][TARGET_COL] \n",
        "  model_lgb = LGBMClassifier(boosting_type = 'gbdt', objective = 'multiclass', metric = 'multi_logloss', n_estimators = 3000, learning_rate = 0.01, \n",
        "                              num_leaves = 45, colsample_bytree = 0.8, subsample = 0.9, subsample_freq = 1, max_depth = 6, reg_alpha = 0.5, reg_lambda = 0.5, \n",
        "                              min_split_gain = 0.05, min_child_weight = 0.05, random_state = random_seed, num_class = 6, silent = -1, verbose = -1)\n",
        "  model_lgb.fit(x_train_, y_train_, eval_set = [(x_train_, y_train_), (x_test_, y_test_)], early_stopping_rounds = 300, eval_metric = 'logloss', verbose = 0)\n",
        "  preds_ = model_lgb.predict_proba(x_test_)\n",
        "  log_loss_ = log_loss(y_test_, preds_)\n",
        "  print(f'LOG_LOSS_{fold + 1}: {log_loss_}')\n",
        "  log_loss_score_lgb.append(log_loss_)\n",
        "  print(\"-\" * 30)\n",
        "print('\\n')\n",
        "print(f\"LOG_LOSS_CV_LGB: {np.mean(log_loss_score_lgb).round(5)}\")"
      ],
      "id": "207c6fb7-82a3-4b28-919c-13f7d7e02bcd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f872d4e-8b48-473a-aa56-1aad92541a59"
      },
      "source": [
        "### HYPERPARAMETER TUNNING USING OPTUNA"
      ],
      "id": "4f872d4e-8b48-473a-aa56-1aad92541a59"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVK_ugOnnQaP"
      },
      "source": [
        "#### CATBOOST"
      ],
      "id": "cVK_ugOnnQaP"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "14609a2d-72cb-44fd-becc-9fd0c5811598"
      },
      "outputs": [],
      "source": [
        "# def objective(trial):\n",
        "#     d_train = Pool(X_train, y_train)\n",
        "#     d_test = Pool(X_test, y_test)\n",
        "#     param = {'l2_leaf_reg' : trial.suggest_float('l2_leaf_reg', 9, 10),\n",
        "#                    'depth' : trial.suggest_int('depth', 7, 9),\n",
        "#                    'learning_rate' : trial.suggest_float('learning_rate', 0.01, 0.02),\n",
        "#                    'n_estimators' : trial.suggest_int('n_estimators ', 3000, 3500),\n",
        "#                    'random_strength' : trial.suggest_float('random_strength', 0.1, 0.2)}\n",
        "\n",
        "#     cat = CatBoostClassifier(**param, bootstrap_type = 'Bayesian', loss_function = 'MultiClass', leaf_estimation_iterations = 1, random_state = random_seed, verbose = 0, use_best_model = True,)\n",
        "#     cat.fit(d_train, eval_set = [(d_test)], verbose = 0, early_stopping_rounds = 500)\n",
        "#     pred = cat.predict_proba(d_test)\n",
        "#     return log_loss(y_test, pred)\n",
        "\n",
        "# study = optuna.create_study(direction = \"minimize\")\n",
        "# study.optimize(objective, n_trials = 50)"
      ],
      "id": "14609a2d-72cb-44fd-becc-9fd0c5811598"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "066b3c25-1082-4590-9474-2a9c9f5d05ed"
      },
      "outputs": [],
      "source": [
        "# trial = study.best_trial\n",
        "# print('LOG_LOSS: {}'.format(trial.value))\n",
        "# print('Best Parameters: {}'.format(trial.params))"
      ],
      "id": "066b3c25-1082-4590-9474-2a9c9f5d05ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9726eea1-d150-48ab-83ca-40a53bbc31b5"
      },
      "source": [
        "### SUBMISSION"
      ],
      "id": "9726eea1-d150-48ab-83ca-40a53bbc31b5"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "bad4068a-f859-41da-ba01-d67aa8ec46de"
      },
      "outputs": [],
      "source": [
        "test.drop('cost_category', axis = 1, inplace = True)\n",
        "def predict_and_submit(test_, filename):\n",
        "    d_ = {'Tour_ID' : sub['Tour_ID'], 'High Cost' : test_[:, 0], 'Higher Cost' : test_[:, 1], 'Highest Cost' : test_[:, 2], 'Low Cost' : test_[:, 3], 'Lower Cost' : test_[:, 4], 'Normal Cost' : test_[:, 5]}\n",
        "    df_ = pd.DataFrame(data = d_)\n",
        "    df_ = df_[['Tour_ID', 'High Cost', 'Higher Cost', 'Highest Cost', 'Low Cost', 'Lower Cost', 'Normal Cost']]\n",
        "    df_.to_csv(f'{path}/{filename}.csv', index = False)\n",
        "    return df_.shape"
      ],
      "id": "bad4068a-f859-41da-ba01-d67aa8ec46de"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "da143bbb-af43-438a-8c1d-4721fd66185c"
      },
      "outputs": [],
      "source": [
        "y_a = cb_model_.predict_proba(test)\n",
        "y_b = lgb_model_.predict_proba(test)\n",
        "y_c = model_cb.predict_proba(test)\n",
        "y_d = model_lgb.predict_proba(test)"
      ],
      "id": "da143bbb-af43-438a-8c1d-4721fd66185c"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuILVqOGCSCZ",
        "outputId": "24abfafe-46a1-44cc-f091-3e0f0e7bdf0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6169, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "pred = (y_a * 0.5 + y_b * 0.5) * 0.5 + (y_c * 0.5 + y_d * 0.5) * 0.5\n",
        "predict_and_submit(pred, 'ai_cat_1.06_')"
      ],
      "id": "LuILVqOGCSCZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### MORE ENSEMBLING"
      ],
      "metadata": {
        "id": "Ajcx3fffyXea"
      },
      "id": "Ajcx3fffyXea"
    },
    {
      "cell_type": "code",
      "source": [
        "a = pd.read_csv(f'{path}/ai_cat_1.06_.csv')\n",
        "b = pd.read_csv(f'{path}/ai_cat_1.06_.csv')\n",
        "c = pd.read_csv(f'{path}/ai_cat_1.06_.csv')\n",
        "a = a.drop('Tour_ID', axis=1)\n",
        "b = b.drop('Tour_ID', axis=1)\n",
        "c = c.drop('Tour_ID', axis=1)"
      ],
      "metadata": {
        "id": "HwNIqx5wpi0Z"
      },
      "id": "HwNIqx5wpi0Z",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stack2 = (0.9 * a + 0.14 * b + 0.1 * c)\n",
        "stack2 = stack2.round(5)\n",
        "predict_and_submit(pred, 'ense210000')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMx-ajcqppm5",
        "outputId": "5ebcc71f-2c27-4b75-a631-8a3e4096ad18"
      },
      "id": "DMx-ajcqppm5",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6169, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        ">Discussion forum: https://zindi.africa/competitions/ai4d-lab-tanzania-tourism-classification-challenge/discussions/12021\n",
        "> External dateset for country to latitude and longitude mapping: <br>https://www.kaggle.com/datasets/paultimothymooney/latitude-and-longitude-for-every-country-and-state<br>\n",
        "> External dateset for country to continent mapping: https://github.com/dbouquin/IS_608/blob/master/NanosatDB_munging/Countries-Continents.csv"
      ],
      "metadata": {
        "id": "lEamUFWDmDCB"
      },
      "id": "lEamUFWDmDCB"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "AI_SQUAD2um__1__working.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}